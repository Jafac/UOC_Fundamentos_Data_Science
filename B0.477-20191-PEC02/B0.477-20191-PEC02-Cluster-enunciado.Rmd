
---
title: "Fundamentos de Data Science: PEC2 - Algoritmos de clustering"
author: "UOC - Master BI - Business Analytics (Nombre Estudiante)"
date: "Noviembre del 2019"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: B0.477-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


******
# Introducción
******

## Descripción de la PEC a realizar
La prueba está estructurada en 7 ejercicios teórico-prácticos que piden que se desarrolle la fase de preparación y estimación de un modelo utilizando un juego de datos.  

Deben responderse al menos 5 de los 7 ejercicios para poder superar la PEC. Para optar a la máxima nota tienen que responderse todos los ejercicios de forma correcta.  

## Criterios de evaluación
**Ejercicios teóricos**  
Todos los ejercicios deben ser presentados de forma razonada y clara. No se aceptará ninguna respuesta que no esté claramente justificada.  

**Ejercicios prácticos**  
Para todas las PEC es necesario documentar en cada ejercicio práctico qué se ha hecho y cómo se ha hecho.  

Pregunta  | Criterio de valoración| Peso
---- | ------------- | ----
1  | Se construye la visualización | 10%
1  | Se interpreta el resultado| 5%
2  | Se entrena el modelo solicitado | 10%
2  | Se describen al menos 4 clusters | 5%
3  | Se razona la respuesta | 10%
3  | Se muestra de forma empírica | 5%
4  | Se realiza la visualización | 5%
4  | Se describe la relación entre las variables analizadas | 5%
5  | Se calculan métricas que permitan la comparación | 5%
5  | Se analizan y comentan los resultados obtenidos  | 5%
5  | Se representa gráficamente  | 5%
6  | Se razona la respuesta | 5%
6  | Se entrena el modelo indicado | 10%
7  | Se muestra la tabla de relaciones entre modelos |10%
7  | Se contesta a la cuestión planteada  | 5%



## Formato y fecha de entega
El formato de entrega es: studentname-PECn.html  
Fecha de Entrega: 15/12/2019  
Se debe entregar la PEC en el buzón de entregas del aula  

******
# Base teórica
******

Esta práctica está basada en los puntos 3.3.1, 3.3.2 y 3.3.3 del material didáctico (Business Analytics) de la asignatura. En el punto 3.3.1 se explica el procedimiento de segmentación jerárquica, mientras en los puntos 3.3.2 y 3.3.3 se explican procedimientos de segmentación no jerárquica para la formación de grupos que, respecto a la información utilizada, sean homogéneos dentro de si mismos y heterogéneos entre unos y otros.   

A lo largo de la práctica se proponen una serie de representaciones gráficas que ayudan a la interpretación de los resultados, sin embargo, podéis insertar más visualizaciones de las propuestas o incluso más código del estrictamente exigido en los ejercicios, eso sí, siempre con el objetivo de completar y mejorar el estudio propuesto.  

En esta práctica importaremos los datos desde un fichero de texto .csv con los campos delimitados por ";". Dichos datos corresponden a la información sobre algunas características de una muestra de asegurados procedentes de una cartera de seguros de automóvil. Los datos han sido extraídos de una cartera de asegurados real, aunque para garantizar la confidencialidad de la información se ha seleccionado una muestra no representativa o sesgada de la realidad. 

******
# Objetivos e información disponible
******

El objetivo de esta segunda PEC se centra en la determinación de distintos perfiles de asegurados del automóvil. 

Las variables que se definen en la base de datos y sus contenidos son:

--poliza: Identificador de póliza

--Sexo: Sexo del cliente

--sri: Situación de riesgo o zona de circulación urbana o no urbana

--gdi: Contratada garantía de daños propios o no

--sin: Número de siniestros en el año analizado

--ant_comp: Antigüedad del cliente en la compañía (en años)

--ant_perm: Antigüedad del permiso de conducir del asegurado (en años)

--edad: Edad del asegurado (en años)

--ant_veh: Antigüedad del vehículo asegurado (en años).


******
# Apartados de la práctica
******
El código R que utilizaremos en la práctica se divide en apartados según las tareas que iremos realizando:  

* Directorio de trabajo

* Importación del fichero de datos .csv. 

* Manipulación y representación de las variables

* Normalización de atributos

* Agrupación jerárquica: Algoritmo aglomerativo

* Uso de la función hclust() para la aglomeración de elementos

* Representación gráfica, Dendograma

* Asignación de los clusters

* Representación de los cluster

* Representación gráfica de variables por cluster

* Agrupación no jerárquica: Algoritmo kmeans

* Uso de la función kmeans() para la formación de cluster (grupos o perfiles de individuos)

* Elección del número de clústers

* Asignación de los clusters

* Representación de los cluster

* Representación gráfica de los clústers
  
* Ejercicios PEC2: Análisis cluster


******
# Directorio de trabajo
******
Antes de pasar a la importación y análisis de los datos definimos un directorio de trabajo o carpeta donde tenéis guardado el fichero de datos. Recordad que si abrís el RStudio desde vuestro directorio de trabajo, pulsando sobre el fichero .RMD que se os proporciona como parte del enunciado, este paso no haría falta.
```{r,eval=TRUE,echo=TRUE}
setwd("C:\\Users\\acamp\\OneDrive\\Documentos\\UOC\\proyectos_git\\UOC_Fundamentos_Data_Science\\B0.477-20191-PEC02")
#Cambiar el argumento de setwd() con vuestro directorio, recordad utilizad las barras /.
```

******
# Importación del fichero de datos .csv. Manipulación y representación de las variables.
******
En primer lugar leemos el fichero de datos con extensión .csv que contiene la información de las 8.088 pólizas analizadas y mostramos su cabecera.
```{r,eval=TRUE,echo=TRUE}
# Lectura de datos
Cartera<-read.table("Datos_analisis_clusters.csv",head=TRUE,sep=";")
head(Cartera)
```
A continuación describimos su contenido con la función summary() y con algunos gráficos. Observamos que para las variables cuantitativas la función summary() proporciona una serie de estadísticos descriptivos relacionados con la posición de la variable (media, mediana, máximo, mínimo,...). Sin embargo, para las variables cualitativas el resultado muestra las frecuencias absolutas (número de casos) de las categorías de las variables.
```{r,eval=TRUE,echo=TRUE}
summary(Cartera)
```

Realizamos algunas representaciones gráficas para describir la base de datos Cartera, utilizamos las herramientas gráficas adecuadas para cada tipo de variable: Cualitativa o Cuantitativa. Recordad que, antes de realizar cualquier análisis, es imprescindible estudiar el comportamiento univariante y bivariante de las variables.
```{r,eval=TRUE,echo=TRUE}
plot(Cartera[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos original", col.main="blue", font.main=1)

freq<-table(Cartera$sin)
freq
barplot(freq,xlab="Número de siniestros", ylab="Frecuencia")
title(main="Número de siniestros", col.main="blue", font.main=1)

table(Cartera$Sexo,Cartera$sin)
prop.table(table(Cartera$Sexo,Cartera$sin))
barplot(prop.table(table(Cartera$Sexo,Cartera$sin)),col=c("darkblue","red"))
legend(5,0.8,c("Hombre","Mujer"),fill = c("darkblue","red"))
```

******
# Normalización de atributos
******
El objetivo es utilizar la información cuantitativa relacionada con la experiencia (edad y ant_perm), con la fidelidad (ant_comp), con el vehículo (ant_veh) y con la siniestralidad (sin) para segmentar a los asegurados. Para ello, en primer lugar, definimos la base de datos con las variables cuantitativas que utilizamos en la segmentación, el resto de variables pueden servir para caracterizar los grupos formados.
```{r,eval=TRUE,echo=TRUE}
clus<-Cartera[,c("sin","ant_comp","ant_perm","edad","ant_veh")]
```

La varianza de las variables (o su rango de valores) utilizadas en el análisis son distintas debido a que miden características diferentes de los individuos y de su vehículo. Por ejemplo, entre las variables utilizadas en el cluster hay algunas que miden el número de años y otra que mide el número de siniestro, es decir, las escalas son muy distintas. Por tanto, antes de iniciar el proceso de segmentación es necesario normalizar los valores de las variables para eliminar el efecto de las distintas escalas de medida, esto equivale a restarles su media y dividirlas por su desviación estándar.

Para la normalización de las variables en la base de datos clus, en primer lugar copiamos su contenido en clus_norm:
```{r,eval=TRUE,echo=TRUE}
clus_norm<-clus
```

Remplazamos las columnas de clus_norm por las columnas de clus normalizadas:
```{r,eval=TRUE,echo=TRUE}
 clus_norm[,c("sin")] <- (clus$sin-mean(clus$sin))/sd(clus$sin)
 clus_norm[,c("ant_comp")] <- (clus$ant_comp-mean(clus$ant_comp))/sd(clus$ant_comp)
 clus_norm[,c("ant_perm")] <- (clus$ant_perm-mean(clus$ant_perm))/sd(clus$ant_perm)
 clus_norm[,c("edad")] <- (clus$edad-mean(clus$edad))/sd(clus$edad)
 clus_norm[,c("ant_veh")] <- (clus$ant_veh-mean(clus$ant_veh))/sd(clus$ant_veh)
```

Realizamos algunas representaciones gráficas para describir las variables normalizadas y comprobamos que la nube de puntos representada es igual a la original, lo único que cambia es la escala de los ejes.
```{r,eval=TRUE,echo=TRUE}
#Normalizadas
plot(clus_norm[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos normalizados", col.main="blue", font.main=1)

#Originales
plot(clus[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia") 
title(main="Nube de puntos originales", col.main="blue", font.main=1)
```

A PARTIR DE AHORA TRABAJAMOS CON LOS DATOS NORMALIZADOS.

******
# Agrupación jerárquica: Algoritmo aglomerativo
******

El algoritmo jerárquico es una técnica no supervisada de agrupación de elementos de forma iterativa. Se comienza con todos los elementos desagrupados y en cada iteración agrupa los dos elementos o grupos de elementos más próximos, utilizando un criterio de enlace, hasta que todos los elementos forman un único grupos.

******
## Uso de la función hclust() para la aglomeración de elementos  
******

Para poder agrupar los elementos es necesario establecer una métrica de distancia siendo la habitual la métrica euclídea. Dado que hay que establecer una distancia las variables deben ser numéricas por lo que si se quieren introducir variables categórica deberá crearse una variable indicador para cada una de las categorías.

```{r,eval=TRUE,echo=TRUE}
distances = dist(clus_norm, method = "euclidean")
```

Una vez considerada la distancia a utilizar y calculada la distancia entre los elementos a agrupar se utiliza el comando hclust de agrupación de elementos

```{r,eval=TRUE,echo=TRUE}
clus_norm_Jerarquico = hclust(distances, method = "ward.D")
```

******
## Representación gráfica, Dendograma   
******

Es importante señalar que no es necesario establecer de forma previa el número de conjuntos o clusters dado que el algoritmo culmina con la agrupación de todos los elementos en un único grupo. 

Esta agrupación se suele representar con un gráfico llamado dendograma que muestra las agrupaciones realizadas (líneas de agrupación) y la distancia entre los elementos o grupos de elementos (altura).

```{r,eval=TRUE,echo=TRUE}
plot(clus_norm_Jerarquico,main="dendograma",xlab="elementos",ylab="distancias")
```

Sobre el propio dendograma se pueden representar los grupos que se obtendrían al separar los elementos en un número k predefinido de grupos.

```{r,eval=TRUE,echo=TRUE}
plot(clus_norm_Jerarquico,main="dendograma",xlab="elementos",ylab="distancias")

rect.hclust(clus_norm_Jerarquico, k=2, border="yellow")
rect.hclust(clus_norm_Jerarquico, k=3, border="blue")
rect.hclust(clus_norm_Jerarquico, k=4, border="green")
rect.hclust(clus_norm_Jerarquico, k=6, border="red")
rect.hclust(clus_norm_Jerarquico, k=10, border="cyan")
```

******
## Asignación de los clusters   
******

Para mantener el conjunto de datos original lo primero que hacemos es crear una copia del conjunto de datos

```{r,eval=TRUE,echo=TRUE}
clus_jerarquico=clus

```

Una vez elegido el número de clusters, se puede asignar a cada elemento el cluster asignado. En este caso vamos a elegir 4 clusters.

```{r,eval=TRUE,echo=TRUE}
NumCluster=4

clus_jerarquico$clusterJerar= cutree(clus_norm_Jerarquico, k = NumCluster)
head(clus_jerarquico)
```

******
## Representación de los cluster  
******
Para poder interpretar los clusters es habitual representarlos mediante el valor medio de las variables en cada cluster.

```{r,eval=TRUE,echo=TRUE}
aggregate(.~clusterJerar,FUN=mean, data=clus_jerarquico)
table(clus_jerarquico$clusterJerar)
```

De esta manera podemos ver que hay diferencias entre los grupos. Por ejemplo el grupo 1 está formado por clientes más jovenes, mientras que el grupo 4 está formado por clientes más veteranos con mucha antigüedad en la compañía. El grupo 3, por su parte, incluye a todos los siniestrados.

******
## Representación gráfica de variables por cluster  
******

Se puede actualizar el gráfico presentado previamente que relaciona la fidelidad con la experiencia incluyendo al asignación de clusters.

```{r,eval=TRUE,echo=TRUE}
plot(clus_jerarquico[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia",col=clus_jerarquico$clusterJerar) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

El grupo 1 es el color negro, el grupo 2 es el color rojo, el grupo 3 es el color verde y el grupo 4 es el color azul.

```{r,eval=TRUE,echo=TRUE}
palette()[1:NumCluster]
```

******
# Agrupación no jerárquica: Algoritmo kmeans
******


******
## Uso de la función kmeans() para la formación de cluster (grupos o perfiles de individuos) 
******
Los algoritmos de segmentación no supervisados, como es el kmeans(), requieren que el analista determine cuál es el número de clústers (grupos) a formar, de hecho, la función kmeans() incorpora como parámetro el número de clústers (centers=).

Para seleccionar el número de grupos podemos utilizar criterios subjetivos o criterios objetivos. Los criterios subjetivos se basan en la visualización de los resultados para determinar el número de clústers más apropiado o en la simple experiencia. A continuación, utilizamos la función kmeans() para formar 3 grupos de individuos y visualizamos algunos resultados como son: los centros de grupos (centers), la suma de cuadrados totales (totss), las sumas de cuadrados dentro de cada grupo y para todos de forma conjunta (withinss y tot.withinss) y la suma de cuadrados entre grupos (betweenss). 

```{r,eval=TRUE,echo=TRUE}
set.seed(123)
modelo_k3<-kmeans(clus_norm,centers=3)
modelo_k3$centers
modelo_k3$totss
modelo_k3$withinss
modelo_k3$tot.withinss
modelo_k3$betweenss
```

******
## Elección del número de clústers   
******
Para la selección del número de clústers también existen criterios objetivos los cuales están basados en la optimización de un criterio de ajuste.

Los criterios de ajustes en el kmeans() se basan en los conceptos de sumas de cuadrados entre grupos (betweens) y dentro de grupos (withins). Hay que tener en cuenta que la suma de cuadrados entre grupos (betweenss) más las sumas de cuadrados dentro de grupos (tot.withinss) nos proporciona la suma de cuadrados totales (tots). Recordad también que las sumas de cuadrados corresponden a los numeradores de las varianzas correspondientes. 

Una segmentación se considera 'óptima' cuando, para cada grupo, los individuos son lo más homgéneos posibles mientras que son más heterogeneos a los individuos del resto de grupos Dicha segmentación coincidirá con aquella que, teniendo un número de grupos razonable, posee una "suma de cuadrados entre grupos"(betweenss) suficientemente grande y, por tanto, una "suma de cuadrados dentro de grupos" (tot.withinss) suficientemente pequeña. Es decir, la varianza dentro de grupos debe ser reducida (individuos dentro de un mismo grupo tiene que ser similares) y la varianza entre grupos debe ser grande (individuos de distintos grupos tienen que ser distintos). También, tenemos que tener en cuenta que a medida que el número de grupos aumenta la suma de cuadrados entre aumenta y, por tanto, la suma de cuadrados dentro disminuye, por tanto, el analista a de decidir cuando el aumento de la suma de cuadrados entre o, alternativamente, la disminución de la suma de cuadrados dentro no son lo suficientemente pronunciados. Por ejemplo, comparamos los resultados para los casos de formar 2 y 3 grupos.

```{r,eval=TRUE,echo=TRUE}
#Suma de cuadrados entre grupos
kmeans(clus_norm,2)$betweenss
kmeans(clus_norm,3)$betweenss

#Suma de cuadrados dentro grupos
kmeans(clus_norm,2)$tot.withinss 
kmeans(clus_norm,3)$tot.withinss

#Suma de cuadrados total
kmeans(clus_norm,2)$totss 
kmeans(clus_norm,3)$totss
```

A continuación, definimos el modo de obtener un gráfico que nos represente la suma de cuadrados entre grupos en función del número de grupos.
```{r,eval=TRUE,echo=TRUE}
set.seed(123)
bss <- kmeans(clus_norm,centers=1)$betweenss
 for (i in 2:10) bss[i] <- kmeans(clus_norm,centers=i)$betweenss

plot(1:10, bss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados entre grupos")
```

******
## Asignación de los clusters   
******

Para mantener el conjunto de datos original lo primero que hacemos es crear una copia del conjunto de datos

```{r,eval=TRUE,echo=TRUE}
clus_kmeans=clus
```

Una vez elegido el número de clusters, se puede asignar a cada elemento el cluster asignado. En este caso vamos a elegir 5 clusters.

```{r,eval=TRUE,echo=TRUE}
NumCluster=5
set.seed(123)
Modelo=kmeans(clus_norm,NumCluster)
clus_kmeans$clusterKmeans= Modelo$cluster
head(clus_kmeans)
```

******
## Representación de los cluster  
******
Para poder interpretar los clusters es habitual representarlos mediante el valor medio de las variables en cada cluster.

```{r,eval=TRUE,echo=TRUE}
aggregate(.~clusterKmeans,FUN=mean, data=clus_kmeans)
table(clus_kmeans$clusterKmeans)
```

De esta manera podemos ver que hay diferencias entre los grupos. Por ejemplo el grupo 5 está formado por clientes más jovenes, mientras que el grupo 1 está formado por clientes  con mucha antigüedad en la compañía. El grupo 3, por su parte, incluye a todos los siniestrados y el grupo 4 recoge a asegurados con una antiguedad del vehículo superior.

******
## Representación gráfica de los clústers   
******

Se puede actualizar el gráfico presentado previamente que relaciona la fidelidad con la experiencia incluyendo al asignación de clusters.

```{r,eval=TRUE,echo=TRUE}
plot(clus_kmeans[c("ant_comp","ant_perm")], xlab="Fidelidad", ylab="Experiencia",col=clus_kmeans$clusterKmeans) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```

El grupo 1 es el color negro, el grupo 2 es el color rojo, el grupo 3 es el color verde, el grupo 4 es el color azul y el grupo 5 es el de color cyan.

```{r,eval=TRUE,echo=TRUE}
palette()[1:NumCluster]
```

******
# Ejercicios PEC2
******

******
## Ejercicio 1
******
En el apartado sobre la *_elección del número de clústers_* del algoritmo kmeans se muestra un gráfico que toma como variable referencia la suma de cuadrados entre '$betweenss'.

Dibuja un gráfico equivalente, tomando como referencia la suma de cuadrados en '$tot.withinss' e interpreta el resultado proponiendo un número de clústers adecuado para el juego de datos.

******
## Respuesta 1
******

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
set.seed(123)
wss <- kmeans(clus_norm,centers=1)$tot.withinss
 for (i in 2:10) wss[i] <- kmeans(clus_norm,centers=i)$tot.withinss

plot(1:10, wss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados dentro de cada grupo (conjunto)")
```

En kmeans, la suma de cuadrados '$tot.withinss' debe ser minima. Pero tambien debemos considerar tener un numero de clusters adecuados (no muy grande).

Se pueden usar varios criterios para definir que numero de clusters cojemos, uno de los mas famosos es el criterio del codo (https://en.wikipedia.org/wiki/Elbow_method_(clustering)). Usaremos mas grupos para ver mejor el "codo".

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
set.seed(123)
wss <- kmeans(clus_norm,centers=1)$tot.withinss
 for (i in 2:20) wss[i] <- kmeans(clus_norm,centers=i)$tot.withinss

plot(1:20, wss, type="l", xlab="Número de grupos",ylab="Sumas de cuadrados dentro de cada grupo (conjunto)")
```

Yo haria una propuesta de 5 clusters.

******
## Ejercicio 2
******
Construir una clusterización en 7 clusters utilizando el algoritmo kmeans. Represente los clusters tal y como se ha hecho en el apartado *_representación de los cluster_* y describa alguno de los grupos (al menos 4)

******
## Respuesta 2
******
Escriba su respuesta aquí

```{r,eval=TRUE,echo=TRUE}
clus_kmeans7=clus
NumCluster7=7
set.seed(123)
Modelo7=kmeans(clus_norm,NumCluster7)
clus_kmeans7$clusterKmeans= Modelo7$cluster
aggregate(.~clusterKmeans,FUN=mean, data=clus_kmeans7)
```

El grupo 1 esta formado por los asegurados más fieles (más años en la compañia).
El grupo 2 esta formado por los conductores más antiguos (más años con el permiso).
El grupo 3 esta formado por la mayoria de siniestrados.
El grupo 4 tiene caracteristicas de los grupos 1, 2, 3 y 5.
El grupo 5 esta formado por los que tienen un vehiculo antiguo.
El grupo 7 esta formado por los asegurados mas jovenes.

******
## Ejercicio 3
******
¿Por qué se ha utilizado el comando set.seed() en el entranamiento del algoritmo kmeans pero no en el entrenamiento del algoritmo jerárquico? Razone la respuesta y acompáñela de ejempolos que la sustenten.

******
## Respuesta 3
******

En el algoritmo jerarquico se empieza con todos los datos y se van agrupando siguiendo una metrica de distancia. Siempre da el mismo resultado. Por otro lado, kmeans empieza eligiendo k centroides de forma aleatoria por eso necesitamos 'set.seed()' para poder reproducir los mismos resultados.


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
set.seed(456)
clus_jerarquico456=clus
distances456 = dist(clus_norm, method = "euclidean")
clus_norm_Jerarquico456 = hclust(distances456, method = "ward.D")
clus_jerarquico456$clusterJerar= cutree(clus_norm_Jerarquico456, k = 7)
table(clus_jerarquico456$clusterJerar)

set.seed(123)
clus_jerarquico456=clus
distances456 = dist(clus_norm, method = "euclidean")
clus_norm_Jerarquico456 = hclust(distances456, method = "ward.D")
clus_jerarquico456$clusterJerar= cutree(clus_norm_Jerarquico456, k = 7)
table(clus_jerarquico456$clusterJerar)
```

Se puede comprobar que los valores de kmeans son iguales aunque cambiemos la 'semilla'.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
set.seed(456)
modelo_k7<-kmeans(clus_norm,centers=7)
modelo_k7$totss
modelo_k7$withinss
modelo_k7$betweenss

set.seed(123)
modelo_k7<-kmeans(clus_norm,centers=7)
modelo_k7$totss
modelo_k7$withinss
modelo_k7$betweenss
```

Se puede comprobar que los valores de kmeans son distintos cuando cambiamos la 'semilla'.
Como curiosidad, la suma de cuadrados totales (totss) es igual en los dos 'semillas' por que el algoritmo hace iteraciones sobre los datos hasta que no puede mejorar la variable 'totss', lo que significa que para esas dos semillas convergen en soluciones "iguales". 
Hablamos de una solución 'local' ya que es la mejor que hemos podido encontrar con la disposición inicial de los centroides. 

******
## Ejercicio 4
******
En el apartado de *_representación gráfica de variables por cluster_* del algorítmo jerárquico se ha dibujado una nube de puntos en función a la fidelidad y la experiencia. Ahora queremos conocer la relación entre la antiguedad del vehículo y la edad representando con 4 colores los clusters construidos con el algorítmo jerárquico.

******
## Respuesta 4
******

Escriba su respuesta aquí

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
plot(clus_jerarquico[c("ant_veh","edad")], xlab="Antiguedad del vehicul", ylab="Edad",col=clus_jerarquico$clusterJerar) 
title(main="Nube de puntos agrupados", col.main="blue", font.main=1)
```


******
## Ejercicio 5
******
En el apartado de Manipulación y representación de variables se ha realizado un análisis gráfico de la relación entre el número de sinistros y el sexo del asegurado. Ahora nos gustaría realizar un análisis de la relación entre el número de sinistros y la contratación o no de garantía de daños propios. ¿Existe alguna relación?. Muestre la representación gráfica y razone la respuesta.

******
## Respuesta 5
******

```{r,eval=TRUE,echo=TRUE}
table(Cartera$gdi,Cartera$sin)
barplot(prop.table(table(Cartera$gdi,Cartera$sin)),col=c("darkblue","red"))
legend(5,0.8,c("Contratada","No contratada"),fill = c("darkblue","red"))
```

No parace haber una relación clara entre el numero de siniestros y la contratación de la garantia de daños propios. Para los siniestros con suficientes datos (0 - 3) la proporcion de contratado / no contratado es de aproximadamente el 70%, por tanto, las variables son independientes.

******
## Ejercicio 6
******
Al presentar los resultados al responsable del negocio, este nos indica que le gustaría incluir la variable sri (Urbano, No urbano) en el análisis. ¿Es posible incluirlo en la segmentación como una variable más?. En caso afirmativo, realizar dicha inclusión y construir una segmentación kmeans con k=4. En caso contrario, indicar el motivo por el que no se puede incluir.

******
## Respuesta 6
******
Escriba su respuesta aquí

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#Escriba su código aquí
```


******
## Ejercicio 7
******
En el apartado de asignación de clusters del algorimto kmeans se ha asignado a cada asegurado un segmento mediante el algoritmo no jerárquico kmeans considerando 5 centroides. ¿Podemos comparar estas asignaciones con las que se obtendrían separando en 5 grupos la segmentación realizada en el apartado Agrupación Jerárquica: Algoritmo Aglomerativo utilizando el algoritmo jerárquico hclust? ¿Son los mismos segmentos? ¿Hay diferencias?. Se pide presentar una tabla en la que se muestren el número de conductores asignados en función a los dos algoritmos (por ejemplo número de clientes asignados al cluster 2 mediante el algoritmo kmeans y al cluster 5 mediante el algoritmo jerárquico)

******
## Respuesta 7
******

Escriba su respuesta aquí

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
#Escriba su código aquí
```

